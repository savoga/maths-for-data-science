\section*{GBM}

Gradient Boosting Method is an agorithm that is part of \hyperref[sec:ensemble-methods]{ensemble methods}.

There are many different implementation of this algorithm; the below one (src: Wikipedia) is one of the most generic and understandable.

\vspace{5mm}

1. Initialize model with constant value

$F_0 = \underset{\hat y}{argmin} \Sigma_{i=1}^n \mathcal{L}(y,\hat y)$

2. for $m = 1$ to $M$:

\hspace{1cm} (A) Compute pseudo-residuals:

\hspace{1cm}$res = - \frac{\partial \mathcal{L}}{\partial \hat y}$

\hspace{1cm} (B) Fit a base learner

\hspace{1cm} (C) For $j=1,...,J_m$ compute $\gamma_{jm} = \underset{\gamma}{argmin}~\Sigma_{x_i \in R_{ij}} \mathcal{L}(y_i, F_{m-1}(x_i)+\gamma)$

\hspace{1cm} (D) Update $F_m(x) = F_ {m-1}(x) + \alpha \Sigma_{j=1}^{J_m} \gamma_m I(x \in R_{jm})$

3. Output $F_M(x)$

\vspace{5mm}

\underline{Step 1: initialize model with constant value}

$$F_0 = \underset{\hat y}{argmin} \Sigma_{i=1}^n \mathcal{L}(y,\hat y)$$

We note that $\hat y$ is constant and doesn't depend on $i$. Consequently, optimizing this equation using SE as loss function will lead to $\hat y = \bar y$:

Note: $MSE := \frac{1}{n}\Sigma_{i=1}^n (y_i - \hat y)^2$ thus $\mathcal{L}: (y,\hat y) \to (y-\hat y)^2$ but we usually choose $\mathcal{L}: (y,\hat y) \to \frac{1}{2}(y-\hat y)^2$\\

$\frac{\partial MSE}{\partial \hat y} = 0$

=> $\frac{2}{n}(y_1 - \hat y) + ... + \frac{2}{n}(y_n - \hat y) = 0$

=> $\hat y = \Sigma_{i=1}^n \frac{y_i}{n} = \bar y$ \\

\underline{Step 2: loop on the number of estimators} \\

\textit{(A) Pseudo-residuals} \\

$$res = - \frac{\partial \mathcal{L}}{\partial \hat y}$$

The gradient of the loss function is $\frac{\partial \mathcal{L}}{\partial \hat y} = -(y - \hat y)$

Residuals as seen in linear regression are typically written as such: $res = y - \hat y$. Thus, gradient boosting uses negative residuals, also called \textbf{pseudo-residuals}. 

Note: pseudo-residuals are just the derivatives of the loss function; that's why we call it \textbf{gradient} boosting. \\

\textit{(B) Fit a weak learner} \\

\lstset{language=Python}
\lstset{frame=lines}
\lstset{caption={Fit base learner}}
\lstset{label={lst:code_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}
h = tree.fit(x, res)
\end{lstlisting}

Weak learners (see definition in \hyperref[sec:adaboost]{Adaboost} section) used in gradient boosting are typically decision trees. These learners are trained on residuals. \\

\textit{(C) Compute a unique output for each leaf} \\

For $j=1,...,J_m$:

$$\gamma_{jm} = \underset{\gamma}{argmin}~\Sigma_{x_i \in R_{ij}} \mathcal{L}(y_i, F_{m-1}(x_i)+\gamma)$$

The loop "For $j=1,...,J_m$" means we iterate on all leafs of the previously built tree.

This step consists in building a unique output for each of its leaf. $R_{j,m}$ are the terminal regions, that is, the leafs of the tree.

If we take the MSE as a loss function, $\gamma_jm$ will be equal to the averages of the leaf outputs. \\

\textit{(D) Update the function} \\

\underline{Step 3: Output the function} \\


Implementation from scratch - comments

Parameters to optimize

\vspace{5mm}